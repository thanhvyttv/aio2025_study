{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3940c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Đọc file PDF\n",
    "FILE_PATH = './data/yolov9.pdf'\n",
    "reader = PdfReader(FILE_PATH)\n",
    "\n",
    "# Lấy số trang\n",
    "num_pages = len(reader.pages)\n",
    "\n",
    "print(num_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4528820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy nội dung trang đầu tiên\n",
    "page_1 = reader.pages[0]\n",
    "page_1_text = page_1.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb12cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9: Learning What You Want to Learn\n",
      "Using Programmable Gradient Information\n",
      "Chien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\n",
      "1Institute of Information Science, Academia Sinica, Taiwan\n",
      "2National Taipei University of Technology, Taiwan\n",
      "3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\n",
      "kinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\n",
      "Abstract\n",
      "Today’s deep learning methods focus on how to design\n",
      "the most appropriate objective functions so that the pre-\n",
      "diction results of the model can be closest to the ground\n",
      "truth. Meanwhile, an appropriate architecture that can\n",
      "facilitate acquisition of enough information for prediction\n",
      "has to be designed. Existing methods ignore a fact that\n",
      "when input data undergoes layer-by-layer feature extrac-\n",
      "tion and spatial transformation, large amount of informa-\n",
      "tion will be lost. This paper will delve into the important is-\n",
      "sues of data loss when data is transmitted through deep net-\n",
      "works, namely information bottleneck and reversible func-\n",
      "tions. We proposed the concept of programmable gradi-\n",
      "ent information (PGI) to cope with the various changes\n",
      "required by deep networks to achieve multiple objectives.\n",
      "PGI can provide complete input information for the tar-\n",
      "get task to calculate objective function, so that reliable\n",
      "gradient information can be obtained to update network\n",
      "weights. In addition, a new lightweight network architec-\n",
      "ture – Generalized Efficient Layer Aggregation Network\n",
      "(GELAN), based on gradient path planning is designed.\n",
      "GELAN’s architecture confirms that PGI has gained su-\n",
      "perior results on lightweight models. We verified the pro-\n",
      "posed GELAN and PGI on MS COCO dataset based ob-\n",
      "ject detection. The results show that GELAN only uses\n",
      "conventional convolution operators to achieve better pa-\n",
      "rameter utilization than the state-of-the-art methods devel-\n",
      "oped based on depth-wise convolution. PGI can be used\n",
      "for variety of models from lightweight to large. It can be\n",
      "used to obtain complete information, so that train-from-\n",
      "scratch models can achieve better results than state-of-the-\n",
      "art models pre-trained using large datasets, the compari-\n",
      "son results are shown in Figure 1. The source codes are at:\n",
      "https://github.com/WongKinYiu/yolov9.\n",
      "1. Introduction\n",
      "Deep learning-based models have demonstrated far bet-\n",
      "ter performance than past artificial intelligence systems in\n",
      "various fields, such as computer vision, language process-\n",
      "ing, and speech recognition. In recent years, researchers\n",
      "Figure 1. Comparisons of the real-time object detecors on MS\n",
      "COCO dataset. The GELAN and PGI-based object detection\n",
      "method surpassed all previous train-from-scratch methods in terms\n",
      "of object detection performance. In terms of accuracy, the new\n",
      "method outperforms RT DETR [43] pre-trained with a large\n",
      "dataset, and it also outperforms depth-wise convolution-based de-\n",
      "sign YOLO MS [7] in terms of parameters utilization.\n",
      "in the field of deep learning have mainly focused on how\n",
      "to develop more powerful system architectures and learn-\n",
      "ing methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\n",
      "formers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\n",
      "56, 81, 81], and Mambas [17, 38, 80]. In addition, some\n",
      "researchers have tried to develop more general objective\n",
      "functions, such as loss function [5, 45, 46, 50, 77, 78], la-\n",
      "bel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\n",
      "sion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\n",
      "all try to precisely find the mapping between input and tar-\n",
      "get tasks. However, most past approaches have ignored that\n",
      "input data may have a non-negligible amount of informa-\n",
      "tion loss during the feedforward process. This loss of in-\n",
      "formation can lead to biased gradient flows, which are sub-\n",
      "sequently used to update the model. The above problems\n",
      "can result in deep networks to establish incorrect associa-\n",
      "tions between targets and inputs, causing the trained model\n",
      "to produce incorrect predictions.\n",
      "1\n",
      "arXiv:2402.13616v2  [cs.CV]  29 Feb 2024\n",
      "Figure 2. Visualization results of random initial weight output feature maps for different network architectures: (a) input image, (b)\n",
      "PlainNet, (c) ResNet, (d) CSPNet, and (e) proposed GELAN. From the figure, we can see that in different architectures, the information\n",
      "provided to the objective function to calculate the loss is lost to varying degrees, and our architecture can retain the most complete\n",
      "information and provide the most reliable gradient information for calculating the objective function.\n",
      "In deep networks, the phenomenon of input data losing\n",
      "information during the feedforward process is commonly\n",
      "known as information bottleneck [59], and its schematic di-\n",
      "agram is as shown in Figure 2. At present, the main meth-\n",
      "ods that can alleviate this phenomenon are as follows: (1)\n",
      "The use of reversible architectures [3, 16, 19]: this method\n",
      "mainly uses repeated input data and maintains the informa-\n",
      "tion of the input data in an explicit way; (2) The use of\n",
      "masked modeling [1, 6, 9, 27, 71, 73]: it mainly uses recon-\n",
      "struction loss and adopts an implicit way to maximize the\n",
      "extracted features and retain the input information; and (3)\n",
      "Introduction of the deep supervision concept [28,51,54,68]:\n",
      "it uses shallow features that have not lost too much impor-\n",
      "tant information to pre-establish a mapping from features\n",
      "to targets to ensure that important information can be trans-\n",
      "ferred to deeper layers. However, the above methods have\n",
      "different drawbacks in the training process and inference\n",
      "process. For example, a reversible architecture requires ad-\n",
      "ditional layers to combine repeatedly fed input data, which\n",
      "will significantly increase the inference cost. In addition,\n",
      "since the input data layer to the output layer cannot have a\n",
      "too deep path, this limitation will make it difficult to model\n",
      "high-order semantic information during the training pro-\n",
      "cess. As for masked modeling, its reconstruction loss some-\n",
      "times conflicts with the target loss. In addition, most mask\n",
      "mechanisms also produce incorrect associations with data.\n",
      "For the deep supervision mechanism, it will produce error\n",
      "accumulation, and if the shallow supervision loses informa-\n",
      "tion during the training process, the subsequent layers will\n",
      "not be able to retrieve the required information. The above\n",
      "phenomenon will be more significant on difficult tasks and\n",
      "small models.\n",
      "To address the above-mentioned issues, we propose a\n",
      "new concept, which is programmable gradient information\n",
      "(PGI). The concept is to generate reliable gradients through\n",
      "auxiliary reversible branch, so that the deep features can\n",
      "still maintain key characteristics for executing target task.\n",
      "The design of auxiliary reversible branch can avoid the se-\n",
      "mantic loss that may be caused by a traditional deep super-\n",
      "vision process that integrates multi-path features. In other\n",
      "words, we are programming gradient information propaga-\n",
      "tion at different semantic levels, and thereby achieving the\n",
      "best training results. The reversible architecture of PGI is\n",
      "built on auxiliary branch, so there is no additional cost.\n",
      "Since PGI can freely select loss function suitable for the\n",
      "target task, it also overcomes the problems encountered by\n",
      "mask modeling. The proposed PGI mechanism can be ap-\n",
      "plied to deep neural networks of various sizes and is more\n",
      "general than the deep supervision mechanism, which is only\n",
      "suitable for very deep neural networks.\n",
      "In this paper, we also designed generalized ELAN\n",
      "(GELAN) based on ELAN [65], the design of GELAN si-\n",
      "multaneously takes into account the number of parameters,\n",
      "computational complexity, accuracy and inference speed.\n",
      "This design allows users to arbitrarily choose appropriate\n",
      "computational blocks for different inference devices. We\n",
      "combined the proposed PGI and GELAN, and then de-\n",
      "signed a new generation of YOLO series object detection\n",
      "system, which we call YOLOv9. We used the MS COCO\n",
      "dataset to conduct experiments, and the experimental results\n",
      "verified that our proposed YOLOv9 achieved the top perfor-\n",
      "mance in all comparisons.\n",
      "We summarize the contributions of this paper as follows:\n",
      "1. We theoretically analyzed the existing deep neural net-\n",
      "work architecture from the perspective of reversible\n",
      "function, and through this process we successfully ex-\n",
      "plained many phenomena that were difficult to explain\n",
      "in the past. We also designed PGI and auxiliary re-\n",
      "versible branch based on this analysis and achieved ex-\n",
      "cellent results.\n",
      "2. The PGI we designed solves the problem that deep su-\n",
      "pervision can only be used for extremely deep neu-\n",
      "ral network architectures, and therefore allows new\n",
      "lightweight architectures to be truly applied in daily\n",
      "life.\n",
      "3. The GELAN we designed only uses conventional con-\n",
      "volution to achieve a higher parameter usage than the\n",
      "depth-wise convolution design that based on the most\n",
      "advanced technology, while showing great advantages\n",
      "of being light, fast, and accurate.\n",
      "4. Combining the proposed PGI and GELAN, the object\n",
      "detection performance of the YOLOv9 on MS COCO\n",
      "dataset greatly surpasses the existing real-time object\n",
      "detectors in all aspects.\n",
      "2\n",
      "2. Related work\n",
      "2.1. Real-time Object Detectors\n",
      "The current mainstream real-time object detectors are the\n",
      "YOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\n",
      "and most of these models use CSPNet [64] or ELAN [65]\n",
      "and their variants as the main computing units. In terms of\n",
      "feature integration, improved PAN [37] or FPN [35] is of-\n",
      "ten used as a tool, and then improved YOLOv3 head [49] or\n",
      "FCOS head [57, 58] is used as prediction head. Recently\n",
      "some real-time object detectors, such as RT DETR [43],\n",
      "which puts its fundation on DETR [4], have also been pro-\n",
      "posed. However, since it is extremely difficult for DETR\n",
      "series object detector to be applied to new domains without\n",
      "a corresponding domain pre-trained model, the most widely\n",
      "used real-time object detector at present is still YOLO se-\n",
      "ries. This paper chooses YOLOv7 [63], which has been\n",
      "proven effective in a variety of computer vision tasks and\n",
      "various scenarios, as a base to develop the proposed method.\n",
      "We use GELAN to improve the architecture and the training\n",
      "process with the proposed PGI. The above novel approach\n",
      "makes the proposed YOLOv9 the top real-time object de-\n",
      "tector of the new generation.\n",
      "2.2. Reversible Architectures\n",
      "The operation unit of reversible architectures [3, 16, 19]\n",
      "must maintain the characteristics of reversible conversion,\n",
      "so it can be ensured that the output feature map of each\n",
      "layer of operation unit can retain complete original informa-\n",
      "tion. Before, RevCol [3] generalizes traditional reversible\n",
      "unit to multiple levels, and in doing so can expand the se-\n",
      "mantic levels expressed by different layer units. Through\n",
      "a literature review of various neural network architectures,\n",
      "we found that there are many high-performing architectures\n",
      "with varying degree of reversible properties. For exam-\n",
      "ple, Res2Net module [11] combines different input parti-\n",
      "tions with the next partition in a hierarchical manner, and\n",
      "concatenates all converted partitions before passing them\n",
      "backwards. CBNet [34, 39] re-introduces the original in-\n",
      "put data through composite backbone to obtain complete\n",
      "original information, and obtains different levels of multi-\n",
      "level reversible information through various composition\n",
      "methods. These network architectures generally have ex-\n",
      "cellent parameter utilization, but the extra composite layers\n",
      "cause slow inference speeds. DynamicDet [36] combines\n",
      "CBNet [34] and the high-efficiency real-time object detec-\n",
      "tor YOLOv7 [63] to achieve a very good trade-off among\n",
      "speed, number of parameters, and accuracy. This paper in-\n",
      "troduces the DynamicDet architecture as the basis for de-\n",
      "signing reversible branches. In addition, reversible infor-\n",
      "mation is further introduced into the proposed PGI. The\n",
      "proposed new architecture does not require additional con-\n",
      "nections during the inference process, so it can fully retain\n",
      "the advantages of speed, parameter amount, and accuracy.\n",
      "2.3. Auxiliary Supervision\n",
      "Deep supervision [28, 54, 68] is the most common auxil-\n",
      "iary supervision method, which performs training by insert-\n",
      "ing additional prediction layers in the middle layers. Es-\n",
      "pecially the application of multi-layer decoders introduced\n",
      "in the transformer-based methods is the most common one.\n",
      "Another common auxiliary supervision method is to utilize\n",
      "the relevant meta information to guide the feature maps pro-\n",
      "duced by the intermediate layers and make them have the\n",
      "properties required by the target tasks [18, 20, 24, 29, 76].\n",
      "Examples of this type include using segmentation loss or\n",
      "depth loss to enhance the accuracy of object detectors. Re-\n",
      "cently, there are many reports in the literature [53, 67, 82]\n",
      "that use different label assignment methods to generate dif-\n",
      "ferent auxiliary supervision mechanisms to speed up the\n",
      "convergence speed of the model and improve the robustness\n",
      "at the same time. However, the auxiliary supervision mech-\n",
      "anism is usually only applicable to large models, so when\n",
      "it is applied to lightweight models, it is easy to cause an\n",
      "under parameterization phenomenon, which makes the per-\n",
      "formance worse. The PGI we proposed designed a way to\n",
      "reprogram multi-level semantic information, and this design\n",
      "allows lightweight models to also benefit from the auxiliary\n",
      "supervision mechanism.\n",
      "3. Problem Statement\n",
      "Usually, people attribute the difficulty of deep neural net-\n",
      "work convergence problem due to factors such as gradient\n",
      "vanish or gradient saturation, and these phenomena do ex-\n",
      "ist in traditional deep neural networks. However, modern\n",
      "deep neural networks have already fundamentally solved\n",
      "the above problem by designing various normalization and\n",
      "activation functions. Nevertheless, deep neural networks\n",
      "still have the problem of slow convergence or poor conver-\n",
      "gence results.\n",
      "In this paper, we explore the nature of the above issue\n",
      "further. Through in-depth analysis of information bottle-\n",
      "neck, we deduced that the root cause of this problem is that\n",
      "the initial gradient originally coming from a very deep net-\n",
      "work has lost a lot of information needed to achieve the\n",
      "goal soon after it is transmitted. In order to confirm this\n",
      "inference, we feedforward deep networks of different archi-\n",
      "tectures with initial weights, and then visualize and illus-\n",
      "trate them in Figure 2. Obviously, PlainNet has lost a lot of\n",
      "important information required for object detection in deep\n",
      "layers. As for the proportion of important information that\n",
      "ResNet, CSPNet, and GELAN can retain, it is indeed posi-\n",
      "tively related to the accuracy that can be obtained after train-\n",
      "ing. We further design reversible network-based methods to\n",
      "solve the causes of the above problems. In this section we\n",
      "shall elaborate our analysis of information bottleneck prin-\n",
      "ciple and reversible functions.\n",
      "3\n",
      "3.1. Information Bottleneck Principle\n",
      "According to information bottleneck principle, we know\n",
      "that data X may cause information loss when going through\n",
      "transformation, as shown in Eq. 1 below:\n",
      "I(X, X) ≥ I(X, fθ(X)) ≥ I(X, gϕ(fθ(X))), (1)\n",
      "where I indicates mutual information, f and g are transfor-\n",
      "mation functions, and θ and ϕ are parameters of f and g,\n",
      "respectively.\n",
      "In deep neural networks, fθ(·) and gϕ(·) respectively\n",
      "represent the operations of two consecutive layers in deep\n",
      "neural network. From Eq. 1, we can predict that as the num-\n",
      "ber of network layer becomes deeper, the original data will\n",
      "be more likely to be lost. However, the parameters of the\n",
      "deep neural network are based on the output of the network\n",
      "as well as the given target, and then update the network after\n",
      "generating new gradients by calculating the loss function.\n",
      "As one can imagine, the output of a deeper neural network\n",
      "is less able to retain complete information about the pre-\n",
      "diction target. This will make it possible to use incomplete\n",
      "information during network training, resulting in unreliable\n",
      "gradients and poor convergence.\n",
      "One way to solve the above problem is to directly in-\n",
      "crease the size of the model. When we use a large number\n",
      "of parameters to construct a model, it is more capable of\n",
      "performing a more complete transformation of the data. The\n",
      "above approach allows even if information is lost during the\n",
      "data feedforward process, there is still a chance to retain\n",
      "enough information to perform the mapping to the target.\n",
      "The above phenomenon explains why the width is more im-\n",
      "portant than the depth in most modern models. However,\n",
      "the above conclusion cannot fundamentally solve the prob-\n",
      "lem of unreliable gradients in very deep neural network.\n",
      "Below, we will introduce how to use reversible functions\n",
      "to solve problems and conduct relative analysis.\n",
      "3.2. Reversible Functions\n",
      "When a function r has an inverse transformation func-\n",
      "tion v, we call this function reversible function, as shown in\n",
      "Eq. 2.\n",
      "X = vζ(rψ(X)), (2)\n",
      "where ψ and ζ are parameters of r and v, respectively. Data\n",
      "X is converted by reversible function without losing infor-\n",
      "mation, as shown in Eq. 3.\n",
      "I(X, X) =I(X, rψ(X)) =I(X, vζ(rψ(X))). (3)\n",
      "When the network’s transformation function is composed\n",
      "of reversible functions, more reliable gradients can be ob-\n",
      "tained to update the model. Almost all of today’s popular\n",
      "deep learning methods are architectures that conform to the\n",
      "reversible property, such as Eq. 4.\n",
      "Xl+1 = Xl + fl+1\n",
      "θ (Xl), (4)\n",
      "where l indicates the l-th layer of a PreAct ResNet and\n",
      "f is the transformation function of the l-th layer. PreAct\n",
      "ResNet [22] repeatedly passes the original data X to sub-\n",
      "sequent layers in an explicit way. Although such a design\n",
      "can make a deep neural network with more than a thousand\n",
      "layers converge very well, it destroys an important reason\n",
      "why we need deep neural networks. That is, for difficult\n",
      "problems, it is difficult for us to directly find simple map-\n",
      "ping functions to map data to targets. This also explains\n",
      "why PreAct ResNet performs worse than ResNet [21] when\n",
      "the number of layers is small.\n",
      "In addition, we tried to use masked modeling that al-\n",
      "lowed the transformer model to achieve significant break-\n",
      "throughs. We use approximation methods, such as Eq. 5,\n",
      "to try to find the inverse transformation v of r, so that the\n",
      "transformed features can retain enough information using\n",
      "sparse features. The form of Eq. 5 is as follows:\n",
      "X = vζ(rψ(X) · M), (5)\n",
      "where M is a dynamic binary mask. Other methods that\n",
      "are commonly used to perform the above tasks are diffusion\n",
      "model and variational autoencoder, and they both have the\n",
      "function of finding the inverse function. However, when\n",
      "we apply the above approach to a lightweight model, there\n",
      "will be defects because the lightweight model will be under\n",
      "parameterized to a large amount of raw data. Because of\n",
      "the above reason, important information I(Y, X) that maps\n",
      "data X to target Y will also face the same problem. For this\n",
      "issue, we will explore it using the concept of information\n",
      "bottleneck [59]. The formula for information bottleneck is\n",
      "as follows:\n",
      "I(X, X) ≥ I(Y, X) ≥ I(Y, fθ(X)) ≥ ... ≥ I(Y, ˆY ). (6)\n",
      "Generally speaking, I(Y, X) will only occupy a very small\n",
      "part of I(X, X). However, it is critical to the target mis-\n",
      "sion. Therefore, even if the amount of information lost in\n",
      "the feedforward stage is not significant, as long as I(Y, X)\n",
      "is covered, the training effect will be greatly affected. The\n",
      "lightweight model itself is in an under parameterized state,\n",
      "so it is easy to lose a lot of important information in the\n",
      "feedforward stage. Therefore, our goal for the lightweight\n",
      "model is how to accurately filterI(Y, X) from I(X, X). As\n",
      "for fully preserving the information of X, that is difficult to\n",
      "achieve. Based on the above analysis, we hope to propose a\n",
      "new deep neural network training method that can not only\n",
      "generate reliable gradients to update the model, but also be\n",
      "suitable for shallow and lightweight neural networks.\n",
      "4\n",
      "Figure 3. PGI and related network architectures and methods. (a) Path Aggregation Network (PAN)) [37], (b) Reversible Columns\n",
      "(RevCol) [3], (c) conventional deep supervision, and (d) our proposed Programmable Gradient Information (PGI). PGI is mainly composed\n",
      "of three components: (1) main branch: architecture used for inference, (2) auxiliary reversible branch: generate reliable gradients to supply\n",
      "main branch for backward transmission, and (3) multi-level auxiliary information: control main branch learning plannable multi-level of\n",
      "semantic information.\n",
      "4. Methodology\n",
      "4.1. Programmable Gradient Information\n",
      "In order to solve the aforementioned problems, we pro-\n",
      "pose a new auxiliary supervision framework called Pro-\n",
      "grammable Gradient Information (PGI), as shown in Fig-\n",
      "ure 3 (d). PGI mainly includes three components, namely\n",
      "(1) main branch, (2) auxiliary reversible branch, and (3)\n",
      "multi-level auxiliary information. From Figure 3 (d) we\n",
      "see that the inference process of PGI only uses main branch\n",
      "and therefore does not require any additional inference cost.\n",
      "As for the other two components, they are used to solve or\n",
      "slow down several important issues in deep learning meth-\n",
      "ods. Among them, auxiliary reversible branch is designed\n",
      "to deal with the problems caused by the deepening of neural\n",
      "networks. Network deepening will cause information bot-\n",
      "tleneck, which will make the loss function unable to gener-\n",
      "ate reliable gradients. As for multi-level auxiliary informa-\n",
      "tion, it is designed to handle the error accumulation problem\n",
      "caused by deep supervision, especially for the architecture\n",
      "and lightweight model of multiple prediction branch. Next,\n",
      "we will introduce these two components step by step.\n",
      "4.1.1 Auxiliary Reversible Branch\n",
      "In PGI, we propose auxiliary reversible branch to gener-\n",
      "ate reliable gradients and update network parameters. By\n",
      "providing information that maps from data to targets, the\n",
      "loss function can provide guidance and avoid the possibil-\n",
      "ity of finding false correlations from incomplete feedfor-\n",
      "ward features that are less relevant to the target. We pro-\n",
      "pose the maintenance of complete information by introduc-\n",
      "ing reversible architecture, but adding main branch to re-\n",
      "versible architecture will consume a lot of inference costs.\n",
      "We analyzed the architecture of Figure 3 (b) and found that\n",
      "when additional connections from deep to shallow layers\n",
      "are added, the inference time will increase by 20%. When\n",
      "we repeatedly add the input data to the high-resolution com-\n",
      "puting layer of the network (yellow box), the inference time\n",
      "even exceeds twice the time.\n",
      "Since our goal is to use reversible architecture to ob-\n",
      "tain reliable gradients, “reversible” is not the only neces-\n",
      "sary condition in the inference stage. In view of this, we\n",
      "regard reversible branch as an expansion of deep supervi-\n",
      "sion branch, and then design auxiliary reversible branch, as\n",
      "shown in Figure 3 (d). As for the main branch deep fea-\n",
      "tures that would have lost important information due to in-\n",
      "formation bottleneck, they will be able to receive reliable\n",
      "gradient information from the auxiliary reversible branch.\n",
      "These gradient information will drive parameter learning to\n",
      "assist in extracting correct and important information, and\n",
      "the above actions can enable the main branch to obtain fea-\n",
      "tures that are more effective for the target task. Moreover,\n",
      "the reversible architecture performs worse on shallow net-\n",
      "works than on general networks because complex tasks re-\n",
      "quire conversion in deeper networks. Our proposed method\n",
      "does not force the main branch to retain complete origi-\n",
      "nal information but updates it by generating useful gradient\n",
      "through the auxiliary supervision mechanism. The advan-\n",
      "tage of this design is that the proposed method can also be\n",
      "applied to shallower networks.\n",
      "5\n",
      "Figure 4. The architecture of GELAN: (a) CSPNet [64], (b) ELAN [65], and (c) proposed GELAN. We imitate CSPNet and extend ELAN\n",
      "into GELAN that can support any computational blocks.\n",
      "Finally, since auxiliary reversible branch can be removed\n",
      "during the inference phase, the inference capabilities of the\n",
      "original network can be retained. We can also choose any\n",
      "reversible architectures in PGI to play the role of auxiliary\n",
      "reversible branch.\n",
      "4.1.2 Multi-level Auxiliary Information\n",
      "In this section we will discuss how multi-level auxiliary in-\n",
      "formation works. The deep supervision architecture includ-\n",
      "ing multiple prediction branch is shown in Figure 3 (c). For\n",
      "object detection, different feature pyramids can be used to\n",
      "perform different tasks, for example together they can de-\n",
      "tect objects of different sizes. Therefore, after connecting\n",
      "to the deep supervision branch, the shallow features will be\n",
      "guided to learn the features required for small object detec-\n",
      "tion, and at this time the system will regard the positions\n",
      "of objects of other sizes as the background. However, the\n",
      "above deed will cause the deep feature pyramids to lose a lot\n",
      "of information needed to predict the target object. Regard-\n",
      "ing this issue, we believe that each feature pyramid needs\n",
      "to receive information about all target objects so that subse-\n",
      "quent main branch can retain complete information to learn\n",
      "predictions for various targets.\n",
      "The concept of multi-level auxiliary information is to in-\n",
      "sert an integration network between the feature pyramid hi-\n",
      "erarchy layers of auxiliary supervision and the main branch,\n",
      "and then uses it to combine returned gradients from differ-\n",
      "ent prediction heads, as shown in Figure 3 (d). Multi-level\n",
      "auxiliary information is then to aggregate the gradient infor-\n",
      "mation containing all target objects, and pass it to the main\n",
      "branch and then update parameters. At this time, the charac-\n",
      "teristics of the main branch’s feature pyramid hierarchy will\n",
      "not be dominated by some specific object’s information. As\n",
      "a result, our method can alleviate the broken information\n",
      "problem in deep supervision. In addition, any integrated\n",
      "network can be used in multi-level auxiliary information.\n",
      "Therefore, we can plan the required semantic levels to guide\n",
      "the learning of network architectures of different sizes.\n",
      "4.2. Generalized ELAN\n",
      "In this Section we describe the proposed new network\n",
      "architecture – GELAN. By combining two neural network\n",
      "architectures, CSPNet [64] and ELAN [65], which are de-\n",
      "signed with gradient path planning, we designed gener-\n",
      "alized efficient layer aggregation network (GELAN) that\n",
      "takes into account lighweight, inference speed, and accu-\n",
      "racy. Its overall architecture is shown in Figure 4. We gen-\n",
      "eralized the capability of ELAN [65], which originally only\n",
      "used stacking of convolutional layers, to a new architecture\n",
      "that can use any computational blocks.\n",
      "5. Experiments\n",
      "5.1. Experimental Setup\n",
      "We verify the proposed method with MS COCO dataset.\n",
      "All experimental setups follow YOLOv7 AF [63], while the\n",
      "dataset is MS COCO 2017 splitting. All models we men-\n",
      "tioned are trained using the train-from-scratch strategy, and\n",
      "the total number of training times is 500 epochs. In setting\n",
      "the learning rate, we use linear warm-up in the first three\n",
      "epochs, and the subsequent epochs set the corresponding\n",
      "decay manner according to the model scale. As for the last\n",
      "15 epochs, we turn mosaic data augmentation off. For more\n",
      "settings, please refer to Appendix.\n",
      "5.2. Implimentation Details\n",
      "We built general and extended version of YOLOv9 based\n",
      "on YOLOv7 [63] and Dynamic YOLOv7 [36] respectively.\n",
      "In the design of the network architecture, we replaced\n",
      "ELAN [65] with GELAN using CSPNet blocks [64] with\n",
      "planned RepConv [63] as computational blocks. We also\n",
      "simplified downsampling module and optimized anchor-\n",
      "free prediction head. As for the auxiliary loss part of PGI,\n",
      "we completely follow YOLOv7’s auxiliary head setting.\n",
      "Please see Appendix for more details.\n",
      "6\n",
      "Table 1. Comparison of state-of-the-art real-time object detectors.\n",
      "Model #Param. (M) FLOPs (G) AP val\n",
      "50:95 (%) AP val\n",
      "50 (%) AP val\n",
      "75 (%) AP val\n",
      "S (%) AP val\n",
      "M (%) AP val\n",
      "L (%)\n",
      "YOLOv5-N r7.0 [14] 1.9 4.5 28.0 45.7 – – – –\n",
      "YOLOv5-S r7.0 [14] 7.2 16.5 37.4 56.8 – – – –\n",
      "YOLOv5-M r7.0 [14] 21.2 49.0 45.4 64.1 – – – –\n",
      "YOLOv5-L r7.0 [14] 46.5 109.1 49.0 67.3 – – – –\n",
      "YOLOv5-X r7.0 [14] 86.7 205.7 50.7 68.9 – – – –\n",
      "YOLOv6-N v3.0 [30] 4.7 11.4 37.0 52.7 – – – –\n",
      "YOLOv6-S v3.0 [30] 18.5 45.3 44.3 61.2 – – – –\n",
      "YOLOv6-M v3.0 [30] 34.9 85.8 49.1 66.1 – – – –\n",
      "YOLOv6-L v3.0 [30] 59.6 150.7 51.8 69.2 – – – –\n",
      "YOLOv7 [63] 36.9 104.7 51.2 69.7 55.9 31.8 55.5 65.0\n",
      "YOLOv7-X [63] 71.3 189.9 52.9 71.1 51.4 36.9 57.7 68.6\n",
      "YOLOv7-N AF [63] 3.1 8.7 37.6 53.3 40.6 18.7 41.7 52.8\n",
      "YOLOv7-S AF [63] 11.0 28.1 45.1 61.8 48.9 25.7 50.2 61.2\n",
      "YOLOv7 AF [63] 43.6 130.5 53.0 70.2 57.5 35.8 58.7 68.9\n",
      "YOLOv8-N [15] 3.2 8.7 37.3 52.6 – – – –\n",
      "YOLOv8-S [15] 11.2 28.6 44.9 61.8 – – – –\n",
      "YOLOv8-M [15] 25.9 78.9 50.2 67.2 – – – –\n",
      "YOLOv8-L [15] 43.7 165.2 52.9 69.8 57.5 35.3 58.3 69.8\n",
      "YOLOv8-X [15] 68.2 257.8 53.9 71.0 58.7 35.7 59.3 70.7\n",
      "DAMO YOLO-T [75] 8.5 18.1 42.0 58.0 45.2 23.0 46.1 58.5\n",
      "DAMO YOLO-S [75] 12.3 37.8 46.0 61.9 49.5 25.9 50.6 62.5\n",
      "DAMO YOLO-M [75] 28.2 61.8 49.2 65.5 53.0 29.7 53.1 66.1\n",
      "DAMO YOLO-L [75] 42.1 97.3 50.8 67.5 55.5 33.2 55.7 66.6\n",
      "Gold YOLO-N [61] 5.6 12.1 39.6 55.7 – 19.7 44.1 57.0\n",
      "Gold YOLO-S [61] 21.5 46.0 45.4 62.5 – 25.3 50.2 62.6\n",
      "Gold YOLO-M [61] 41.3 87.5 49.8 67.0 – 32.3 55.3 66.3\n",
      "Gold YOLO-L [61] 75.1 151.7 51.8 68.9 – 34.1 57.4 68.2\n",
      "YOLO MS-N [7] 4.5 17.4 43.4 60.4 47.6 23.7 48.3 60.3\n",
      "YOLO MS-S [7] 8.1 31.2 46.2 63.7 50.5 26.9 50.5 63.0\n",
      "YOLO MS [7] 22.2 80.2 51.0 68.6 55.7 33.1 56.1 66.5\n",
      "GELAN-S (Ours) 7.1 26.4 46.7 63.0 50.7 25.9 51.5 64.0\n",
      "GELAN-M (Ours) 20.0 76.3 51.1 67.9 55.7 33.6 56.4 67.3\n",
      "GELAN-C (Ours) 25.3 102.1 52.5 69.5 57.3 35.8 57.6 69.4\n",
      "GELAN-E (Ours) 57.3 189.0 55.0 71.9 60.0 38.0 60.6 70.9\n",
      "YOLOv9-S (Ours) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\n",
      "YOLOv9-M (Ours) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\n",
      "YOLOv9-C (Ours) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\n",
      "YOLOv9-E (Ours) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\n",
      "5.3. Comparison with state-of-the-arts\n",
      "Table 1 lists comparison of our proposed YOLOv9 with\n",
      "other train-from-scratch real-time object detectors. Over-\n",
      "all, the best performing methods among existing methods\n",
      "are YOLO MS-S [7] for lightweight models, YOLO MS [7]\n",
      "for medium models, YOLOv7 AF [63] for general mod-\n",
      "els, and YOLOv8-X [15] for large models. Compared with\n",
      "lightweight and medium model YOLO MS [7], YOLOv9\n",
      "has about 10% less parameters and 5 ∼15% less calcula-\n",
      "tions, but still has a 0.4 ∼0.6% improvement in AP. Com-\n",
      "pared with YOLOv7 AF, YOLOv9-C has 42% less pa-\n",
      "rameters and 22% less calculations, but achieves the same\n",
      "AP (53%). Compared with YOLOv8-X, YOLOv9-E has\n",
      "16% less parameters, 27% less calculations, and has sig-\n",
      "nificant improvement of 1.7% AP. The above comparison\n",
      "results show that our proposed YOLOv9 has significantly\n",
      "improved in all aspects compared with existing methods.\n",
      "On the other hand, we also include ImageNet pretrained\n",
      "model in the comparison, and the results are shown in Fig-\n",
      "ure 5. We compare them based on the parameters and the\n",
      "amount of computation respectively. In terms of the num-\n",
      "ber of parameters, the best performing large model is RT\n",
      "DETR [43]. From Figure 5, we can see that YOLOv9 using\n",
      "conventional convolution is even better than YOLO MS us-\n",
      "ing depth-wise convolution in parameter utilization. As for\n",
      "the parameter utilization of large models, it also greatly sur-\n",
      "passes RT DETR using ImageNet pretrained model. Even\n",
      "better is that in the deep model, YOLOv9 shows the huge\n",
      "advantages of using PGI. By accurately retaining and ex-\n",
      "tracting the information needed to map the data to the tar-\n",
      "get, our method requires only 66% of the parameters while\n",
      "maintaining the accuracy as RT DETR-X.\n",
      "7\n",
      "Figure 5. Comparison of state-of-the-art real-time object detectors. The methods participating in the comparison all use ImageNet as\n",
      "pre-trained weights, including RT DETR [43], RTMDet [44], and PP-YOLOE [74], etc. The YOLOv9 that uses train-from-scratch method\n",
      "clearly surpasses the performance of other methods.\n",
      "As for the amount of computation, the best existing mod-\n",
      "els from the smallest to the largest are YOLO MS [7], PP\n",
      "YOLOE [74], and RT DETR [43]. From Figure 5, we can\n",
      "see that YOLOv9 is far superior to the train-from-scratch\n",
      "methods in terms of computational complexity. In addi-\n",
      "tion, if compared with those based on depth-wise convo-\n",
      "lution and ImageNet-based pretrained models, YOLOv9 is\n",
      "also very competitive.\n",
      "5.4. Ablation Studies\n",
      "5.4.1 Generalized ELAN\n",
      "For GELAN, we first do ablation studies for computational\n",
      "blocks. We used Res blocks [21], Dark blocks [49], and\n",
      "CSP blocks [64] to conduct experiments, respectively. Ta-\n",
      "ble 2 shows that after replacing convolutional layers in\n",
      "ELAN with different computational blocks, the system can\n",
      "maintain good performance. Users are indeed free to re-\n",
      "place computational blocks and use them on their respective\n",
      "inference devices. Among different computational block re-\n",
      "placements, CSP blocks perform particularly well. They\n",
      "not only reduce the amount of parameters and computation,\n",
      "but also improve AP by 0.7%. Therefore, we choose CSP-\n",
      "ELAN as the component unit of GELAN in YOLOv9.\n",
      "Table 2. Ablation study on various computational blocks.\n",
      "Model CB type #Param. FLOPs AP val\n",
      "50:95\n",
      "GELAN-S Conv 6.2M 23.5G 44.8%\n",
      "GELAN-S Res [21] 5.4M 21.0G 44.3%\n",
      "GELAN-S Dark [49] 5.7M 21.8G 44.5%\n",
      "GELAN-S CSP [64] 5.9M 22.4G 45.5%\n",
      "1 CB type nedotes as computational block type.\n",
      "2 -S nedotes small size model.\n",
      "Next, we conduct ELAN block-depth and CSP block-\n",
      "depth experiments on GELAN of different sizes, and dis-\n",
      "play the results in Table 3. We can see that when the depth\n",
      "of ELAN is increased from 1 to 2, the accuracy is signif-\n",
      "icantly improved. But when the depth is greater than or\n",
      "equal to 2, no matter it is improving the ELAN depth or the\n",
      "CSP depth, the number of parameters, the amount of com-\n",
      "putation, and the accuracy will always show a linear rela-\n",
      "tionship. This means GELAN is not sensitive to the depth.\n",
      "In other words, users can arbitrarily combine the compo-\n",
      "nents in GELAN to design the network architecture, and\n",
      "have a model with stable performance without special de-\n",
      "sign. In Table 3, for YOLOv9-{S,M,C}, we set the pairing\n",
      "of the ELAN depth and the CSP depth to {{2, 3}, {2, 1},\n",
      "{2, 1}}.\n",
      "Table 3. Ablation study on ELAN and CSP depth.\n",
      "Model D ELAN DCSP #Param. FLOPs AP val\n",
      "50:95\n",
      "GELAN-S 2 1 5.9M 22.4G 45.5%\n",
      "GELAN-S 2 2 6.5M 24.4G 46.0%\n",
      "GELAN-S 3 1 7.1M 26.3G 46.5%\n",
      "GELAN-S 2 3 7.1M 26.4G 46.7%\n",
      "GELAN-M 2 1 20.0M 76.3G 51.1%\n",
      "GELAN-M 2 2 22.2M 85.1G 51.7%\n",
      "GELAN-M 3 1 24.3M 93.5G 51.8%\n",
      "GELAN-M 2 3 24.4M 94.0G 52.3%\n",
      "GELAN-C 1 1 18.9M 77.5G 50.7%\n",
      "GELAN-C 2 1 25.3M 102.1G 52.5%\n",
      "GELAN-C 2 2 28.6M 114.4G 53.0%\n",
      "GELAN-C 3 1 31.7M 126.8G 53.2%\n",
      "GELAN-C 2 3 31.9M 126.7G 53.3%\n",
      "1 DELAN and DCSP respectively nedotes depth of ELAN and CSP.\n",
      "2 -{S, M, C} indicate small, medium, and compact models.\n",
      "8\n",
      "5.4.2 Programmable Gradient Information\n",
      "In terms of PGI, we performed ablation studies on auxiliary\n",
      "reversible branch and multi-level auxiliary information on\n",
      "the backbone and neck, respectively. We designed auxiliary\n",
      "reversible branch ICN to use DHLC [34] linkage to obtain\n",
      "multi-level reversible information. As for multi-level aux-\n",
      "iliary information, we use FPN and PAN for ablation stud-\n",
      "ies and the role of PFH is equivalent to the traditional deep\n",
      "supervision. The results of all experiments are listed in Ta-\n",
      "ble 4. From Table 4, we can see that PFH is only effective in\n",
      "deep models, while our proposed PGI can improve accuracy\n",
      "under different combinations. Especially when using ICN,\n",
      "we get stable and better results. We also tried to apply the\n",
      "lead-head guided assignment proposed in YOLOv7 [63] to\n",
      "the PGI’s auxiliary supervision, and achieved much better\n",
      "performance.\n",
      "Table 4. Ablation study on PGI of backbone and neck.\n",
      "Model G backbone Gneck APval\n",
      "50:95 APval\n",
      "S APval\n",
      "M APval\n",
      "L\n",
      "GELAN-C – – 52.5% 35.8% 57.6% 69.4%\n",
      "GELAN-C PFH – 52.5% 35.3% 58.1% 68.9%\n",
      "GELAN-C FPN – 52.6% 35.3% 58.1% 68.9%\n",
      "GELAN-C – ICN 52.7% 35.3% 58.4% 68.9%\n",
      "GELAN-C FPN ICN 52.8% 35.8% 58.2% 69.1%\n",
      "GELAN-C ICN – 52.9% 35.2% 58.7% 68.6%\n",
      "GELAN-C LHG-ICN – 53.0% 36.3% 58.5% 69.1%\n",
      "GELAN-E – – 55.0% 38.0% 60.6% 70.9%\n",
      "GELAN-E PFH – 55.3% 38.3% 60.3% 71.6%\n",
      "GELAN-E FPN – 55.6% 40.2% 61.0% 71.4%\n",
      "GELAN-E PAN – 55.5% 39.0% 61.1% 71.5%\n",
      "GELAN-E FPN ICN 55.6% 39.8% 60.9% 71.9%\n",
      "1 DELAN and DCSP respectively nedotes depth of ELAN and CSP.\n",
      "2 LHG indicates lead head guided training proposed by YOLOv7 [63].\n",
      "We further implemented the concepts of PGI and deep\n",
      "supervision on models of various sizes and compared the\n",
      "results, these results are shown in Table 5. As analyzed at\n",
      "the beginning, introduction of deep supervision will cause\n",
      "a loss of accuracy for shallow models. As for general mod-\n",
      "els, introducing deep supervision will cause unstable perfor-\n",
      "mance, and the design concept of deep supervision can only\n",
      "bring gains in extremely deep models. The proposed PGI\n",
      "can effectively handle problems such as information bottle-\n",
      "neck and information broken, and can comprehensively im-\n",
      "prove the accuracy of models of different sizes. The concept\n",
      "of PGI brings two valuable contributions. The first one is to\n",
      "make the auxiliary supervision method applicable to shal-\n",
      "low models, while the second one is to make the deep model\n",
      "training process obtain more reliable gradients. These gra-\n",
      "dients enable deep models to use more accurate information\n",
      "to establish correct correlations between data and targets.\n",
      "Table 5. Ablation study on PGI.\n",
      "Model AP val\n",
      "50:95 APval\n",
      "50 APval\n",
      "75\n",
      "GELAN-S 46.7% 63.0% 50.7%\n",
      "+ DS 46.5% -0.2 62.9% -0.1 50.5% -0.2\n",
      "+ PGI 46.8% +0.1 63.4% +0.4 50.7% =\n",
      "GELAN-M 51.1% 67.9% 55.7%\n",
      "+ DS 51.2% +0.1 68.2% +0.3 55.7% =\n",
      "+ PGI 51.4% +0.3 68.1% +0.2 56.1% +0.4\n",
      "GELAN-C 52.5% 69.5% 57.3%\n",
      "+ DS 52.5% = 69.9% +0.4 57.1% -0.2\n",
      "+ PGI 53.0% +0.5 70.3% +0.8 57.8% +0.5\n",
      "GELAN-E 55.0% 71.9% 60.0%\n",
      "+ DS 55.3% +0.3 72.3% +0.4 60.2% +0.2\n",
      "+ PGI 55.6% +0.6 72.8% +0.9 60.6% +0.6\n",
      "1 DS indicates deep supervision.\n",
      "2 -{S, M, C, E} indicate small, medium, compact, and extended models.\n",
      "Finally, we show in the table the results of gradually in-\n",
      "creasing components from baseline YOLOv7 to YOLOv9-\n",
      "E. The GELAN and PGI we proposed have brought all-\n",
      "round improvement to the model.\n",
      "Table 6. Ablation study on GELAN and PGI.\n",
      "Model #Param. FLOPs AP val\n",
      "50:95 APval\n",
      "S APval\n",
      "M APval\n",
      "L\n",
      "YOLOv7 [63] 36.9 104.7 51.2% 31.8% 55.5% 65.0%\n",
      "+ AF [63] 43.6 130.5 53.0% 35.8% 58.7% 68.9%\n",
      "+ GELAN 41.2 126.4 53.2% 36.2% 58.5% 69.9%\n",
      "+ DHLC [34] 57.3 189.0 55.0% 38.0% 60.6% 70.9%\n",
      "+ PGI 57.3 189.0 55.6% 40.2% 61.0% 71.4%\n",
      "5.5. Visualization\n",
      "This section will explore the information bottleneck is-\n",
      "sues and visualize them. In addition, we will also visualize\n",
      "how the proposed PGI uses reliable gradients to find the\n",
      "correct correlations between data and targets. In Figure 6\n",
      "we show the visualization results of feature maps obtained\n",
      "by using random initial weights as feedforward under dif-\n",
      "ferent architectures. We can see that as the number of lay-\n",
      "ers increases, the original information of all architectures\n",
      "gradually decreases. For example, at the 50 th layer of the\n",
      "PlainNet, it is difficult to see the location of objects, and all\n",
      "distinguishable features will be lost at the 100 th layer. As\n",
      "for ResNet, although the position of object can still be seen\n",
      "at the 50 th layer, the boundary information has been lost.\n",
      "When the depth reached to the 100th layer, the whole image\n",
      "becomes blurry. Both CSPNet and the proposed GELAN\n",
      "perform very well, and they both can maintain features that\n",
      "support clear identification of objects until the 200 th layer.\n",
      "Among the comparisons, GELAN has more stable results\n",
      "and clearer boundary information.\n",
      "9\n",
      "Figure 6. Feature maps (visualization results) output by random initial weights of PlainNet, ResNet, CSPNet, and GELAN at different\n",
      "depths. After 100 layers, ResNet begins to produce feedforward output that is enough to obfuscate object information. Our proposed\n",
      "GELAN can still retain quite complete information up to the 150th layer, and is still sufficiently discriminative up to the 200th layer.\n",
      "Figure 7. PAN feature maps (visualization results) of GELAN\n",
      "and YOLOv9 (GELAN + PGI) after one epoch of bias warm-up.\n",
      "GELAN originally had some divergence, but after adding PGI’s\n",
      "reversible branch, it is more capable of focusing on the target ob-\n",
      "ject.\n",
      "Figure 7 is used to show whether PGI can provide more\n",
      "reliable gradients during the training process, so that the\n",
      "parameters used for updating can effectively capture the\n",
      "relationship between the input data and the target. Fig-\n",
      "ure 7 shows the visualization results of the feature map of\n",
      "GELAN and YOLOv9 (GELAN + PGI) in PAN bias warm-\n",
      "up. From the comparison of Figure 7(b) and (c), we can\n",
      "clearly see that PGI accurately and concisely captures the\n",
      "area containing objects. As for GELAN that does not use\n",
      "PGI, we found that it had divergence when detecting ob-\n",
      "ject boundaries, and it also produced unexpected responses\n",
      "in some background areas. This experiment confirms that\n",
      "PGI can indeed provide better gradients to update parame-\n",
      "ters and enable the feedforward stage of the main branch to\n",
      "retain more important features.\n",
      "6. Conclusions\n",
      "In this paper, we propose to use PGI to solve the infor-\n",
      "mation bottleneck problem and the problem that the deep\n",
      "supervision mechanism is not suitable for lightweight neu-\n",
      "ral networks. We designed GELAN, a highly efficient\n",
      "and lightweight neural network. In terms of object detec-\n",
      "tion, GELAN has strong and stable performance at different\n",
      "computational blocks and depth settings. It can indeed be\n",
      "widely expanded into a model suitable for various inference\n",
      "devices. For the above two issues, the introduction of PGI\n",
      "allows both lightweight models and deep models to achieve\n",
      "significant improvements in accuracy. The YOLOv9, de-\n",
      "signed by combining PGI and GELAN, has shown strong\n",
      "competitiveness. Its excellent design allows the deep model\n",
      "to reduce the number of parameters by 49% and the amount\n",
      "of calculations by 43% compared with YOLOv8, but it still\n",
      "has a 0.6% AP improvement on MS COCO dataset.\n",
      "7. Acknowledgements\n",
      "The authors wish to thank National Center for High-\n",
      "performance Computing (NCHC) for providing computa-\n",
      "tional and storage resources.\n",
      "10\n",
      "References\n",
      "[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT:\n",
      "BERT pre-training of image transformers. In International\n",
      "Conference on Learning Representations (ICLR), 2022. 2\n",
      "[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\n",
      "Yuan Mark Liao. YOLOv4: Optimal speed and accuracy of\n",
      "object detection. arXiv preprint arXiv:2004.10934, 2020. 3\n",
      "[3] Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiang-\n",
      "wen Kong, Jun Li, and Xiangyu Zhang. Reversible column\n",
      "networks. In International Conference on Learning Repre-\n",
      "sentations (ICLR), 2023. 2, 3, 5\n",
      "[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\n",
      "Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-\n",
      "to-end object detection with transformers. In Proceedings\n",
      "of the European Conference on Computer Vision (ECCV) ,\n",
      "pages 213–229, 2020. 3\n",
      "[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\n",
      "Junni Zou. AP-loss for accurate one-stage object detection.\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelli-\n",
      "gence (TPAMI), 43(11):3782–3798, 2020. 1\n",
      "[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\n",
      "Wenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\n",
      "distillated masked autoencoder. In Proceedings of the Euro-\n",
      "pean Conference on Computer Vision (ECCV) , pages 108–\n",
      "124, 2022. 2\n",
      "[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\n",
      "Hou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\n",
      "scale representation learning for real-time object detection.\n",
      "arXiv preprint arXiv:2308.05480, 2023. 1, 3, 7, 8\n",
      "[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\n",
      "Wang, and Lu Yuan. DaVIT: Dual attention vision trans-\n",
      "formers. In Proceedings of the European Conference on\n",
      "Computer Vision (ECCV), pages 74–92, 2022. 1\n",
      "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n",
      "vain Gelly, et al. An image is worth 16x16 words: Trans-\n",
      "formers for image recognition at scale. InInternational Con-\n",
      "ference on Learning Representations (ICLR), 2021. 1, 2\n",
      "[10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\n",
      "and Weilin Huang. TOOD: Task-aligned one-stage object\n",
      "detection. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision (ICCV), pages 3490–3499,\n",
      "2021. 1\n",
      "[11] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\n",
      "Zhang, Ming-Hsuan Yang, and Philip Torr. Res2Net: A\n",
      "new multi-scale backbone architecture. IEEE Transac-\n",
      "tions on Pattern Analysis and Machine Intelligence (TPAMI),\n",
      "43(2):652–662, 2019. 3\n",
      "[12] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\n",
      "Sun. OTA: Optimal transport assignment for object detec-\n",
      "tion. In Proceedings of the IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition (CVPR) , pages 303–\n",
      "312, 2021. 1\n",
      "[13] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\n",
      "Sun. YOLOX: Exceeding YOLO series in 2021. arXiv\n",
      "preprint arXiv:2107.08430, 2021. 3\n",
      "[14] Jocher Glenn. YOLOv5 release v7.0. https://github.\n",
      "com/ultralytics/yolov5/releases/tag/v7.\n",
      "0, 2022. 3, 7\n",
      "[15] Jocher Glenn. YOLOv8 release v8.1.0. https :\n",
      "/ / github . com / ultralytics / ultralytics /\n",
      "releases/tag/v8.1.0, 2024. 3, 7\n",
      "[16] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\n",
      "Grosse. The reversible residual network: Backpropagation\n",
      "without storing activations. Advances in Neural Information\n",
      "Processing Systems (NeurIPS), 2017. 2, 3\n",
      "[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\n",
      "modeling with selective state spaces. arXiv preprint\n",
      "arXiv:2312.00752, 2023. 1\n",
      "[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\n",
      "Chunhong Pan. AugFPN: Improving multi-scale fea-\n",
      "ture learning for object detection. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 12595–12604, 2020. 1, 3\n",
      "[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\n",
      "ploring disentangled representations in masked image mod-\n",
      "eling. Advances in Neural Information Processing Systems\n",
      "(NeurIPS), 2023. 2, 3\n",
      "[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\n",
      "Boundary-aware instance segmentation. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 5696–5704, 2017. 1, 3\n",
      "[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Deep residual learning for image recognition. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 8\n",
      "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Identity mappings in deep residual networks. InProceedings\n",
      "of the European Conference on Computer Vision (ECCV) ,\n",
      "pages 630–645. Springer, 2016. 1, 4\n",
      "[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\n",
      "ian Q Weinberger. Densely connected convolutional net-\n",
      "works. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "4700–4708, 2017. 1\n",
      "[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\n",
      "ston H Hsu. MonoDTR: Monocular 3D object detection with\n",
      "depth-aware transformer. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 4012–4021, 2022. 1, 3\n",
      "[25] Lin Huang, Weisheng Li, Linlin Shen, Haojie Fu, Xue Xiao,\n",
      "and Suihan Xiao. YOLOCS: Object detection based on dense\n",
      "channel compression for feature spatial solidification. arXiv\n",
      "preprint arXiv:2305.04170, 2023. 3\n",
      "[26] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,\n",
      "Andrew Zisserman, and Joao Carreira. Perceiver: General\n",
      "perception with iterative attention. In International Confer-\n",
      "ence on Machine Learning (ICML), pages 4651–4664, 2021.\n",
      "1\n",
      "[27] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\n",
      "Toutanova. BERT: Pre-training of deep bidirectional trans-\n",
      "formers for language understanding. In Proceedings of\n",
      "NAACL-HLT, volume 1, page 2, 2019. 2\n",
      "11\n",
      "[28] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\n",
      "Zhang, and Zhuowen Tu. Deeply-supervised nets. In Ar-\n",
      "tificial Intelligence and Statistics , pages 562–570, 2015. 1,\n",
      "2, 3\n",
      "[29] Alex Levinshtein, Alborz Rezazadeh Sereshkeh, and Kon-\n",
      "stantinos Derpanis. DATNet: Dense auxiliary tasks for ob-\n",
      "ject detection. In Proceedings of the IEEE/CVF Winter Con-\n",
      "ference on Applications of Computer Vision (WACV), pages\n",
      "1419–1427, 2020. 1, 3\n",
      "[30] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\n",
      "Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-\n",
      "iang Chu. YOLOv6 v3.0: A full-scale reloading. arXiv\n",
      "preprint arXiv:2301.05586, 2023. 3, 7, 2, 4\n",
      "[31] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei\n",
      "Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\n",
      "Weiqiang Nie, et al. YOLOv6: A single-stage object de-\n",
      "tection framework for industrial applications. arXiv preprint\n",
      "arXiv:2209.02976, 2022. 3\n",
      "[32] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng\n",
      "Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,\n",
      "Wenhai Wang, et al. Uni-perceiver v2: A generalist model\n",
      "for large-scale vision and vision-language tasks. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), pages 2691–2700, 2023. 1\n",
      "[33] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\n",
      "dual weighting label assignment scheme for object detection.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR) , pages 9387–9396,\n",
      "2022. 1\n",
      "[34] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\n",
      "Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\n",
      "Net: A composite backbone network architecture for object\n",
      "detection. IEEE Transactions on Image Processing (TIP) ,\n",
      "2022. 3, 9\n",
      "[35] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\n",
      "Bharath Hariharan, and Serge Belongie. Feature pyra-\n",
      "mid networks for object detection. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 2117–2125, 2017. 3\n",
      "[36] Zhihao Lin, Yongtao Wang, Jinhe Zhang, and Xiaojie Chu.\n",
      "DynamicDet: A unified dynamic architecture for object de-\n",
      "tection. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "6282–6291, 2023. 3, 6, 2, 4\n",
      "[37] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\n",
      "Path aggregation network for instance segmentation. In Pro-\n",
      "ceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition (CVPR) , pages 8759–8768, 2018.\n",
      "3, 5\n",
      "[38] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi\n",
      "Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba:\n",
      "Visual state space model. arXiv preprint arXiv:2401.10166,\n",
      "2024. 1\n",
      "[39] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\n",
      "Qijie Zhao, Zhi Tang, and Haibin Ling. CBNet: A novel\n",
      "composite backbone network architecture for object detec-\n",
      "tion. In Proceedings of the AAAI Conference on Artificial\n",
      "Intelligence (AAAI), pages 11653–11660, 2020. 3\n",
      "[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\n",
      "Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\n",
      "Swin transformer v2: Scaling up capacity and resolution. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vi-\n",
      "sion and Pattern Recognition (CVPR), 2022. 1\n",
      "[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\n",
      "Zhang, Stephen Lin, and Baining Guo. Swin transformer:\n",
      "Hierarchical vision transformer using shifted windows. In\n",
      "Proceedings of the IEEE/CVF International Conference on\n",
      "Computer Vision (ICCV), pages 10012–10022, 2021. 1\n",
      "[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\n",
      "enhofer, Trevor Darrell, and Saining Xie. A ConvNet for the\n",
      "2020s. In Proceedings of the IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition (CVPR), pages 11976–\n",
      "11986, 2022. 1\n",
      "[43] Wenyu Lv, Shangliang Xu, Yian Zhao, Guanzhong Wang,\n",
      "Jinman Wei, Cheng Cui, Yuning Du, Qingqing Dang, and\n",
      "Yi Liu. DETRs beat YOLOs on real-time object detection.\n",
      "arXiv preprint arXiv:2304.08069, 2023. 1, 3, 7, 8, 2, 4\n",
      "[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\n",
      "Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\n",
      "RTMDet: An empirical study of designing real-time object\n",
      "detectors. arXiv preprint arXiv:2212.07784, 2022. 8, 2, 3, 4\n",
      "[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\n",
      "Kalkan. A ranking-based, balanced loss function unify-\n",
      "ing classification and localisation in object detection. Ad-\n",
      "vances in Neural Information Processing Systems (NeurIPS),\n",
      "33:15534–15545, 2020. 1\n",
      "[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\n",
      "Kalkan. Rank & sort loss for object detection and instance\n",
      "segmentation. In Proceedings of the IEEE/CVF Interna-\n",
      "tional Conference on Computer Vision (ICCV), pages 3009–\n",
      "3018, 2021. 1\n",
      "[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\n",
      "Farhadi. You only look once: Unified, real-time object detec-\n",
      "tion. In Proceedings of the IEEE/CVF Conference on Com-\n",
      "puter Vision and Pattern Recognition (CVPR) , pages 779–\n",
      "788, 2016. 3\n",
      "[48] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\n",
      "stronger. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "7263–7271, 2017. 3\n",
      "[49] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\n",
      "improvement. arXiv preprint arXiv:1804.02767, 2018. 3, 8\n",
      "[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\n",
      "Sadeghian, Ian Reid, and Silvio Savarese. Generalized in-\n",
      "tersection over union: A metric and a loss for bounding box\n",
      "regression. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), pages\n",
      "658–666, 2019. 1\n",
      "[51] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\n",
      "Yurong Chen, and Xiangyang Xue. Object detection from\n",
      "scratch with deep supervision. IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence (TPAMI), 42(2):398–412,\n",
      "2019. 1, 2\n",
      "[52] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiver-\n",
      "actor: A multi-task transformer for robotic manipulation.\n",
      "12\n",
      "In Conference on Robot Learning (CoRL) , pages 785–799,\n",
      "2023. 1\n",
      "[53] Peize Sun, Yi Jiang, Enze Xie, Wenqi Shao, Zehuan Yuan,\n",
      "Changhu Wang, and Ping Luo. What makes for end-to-end\n",
      "object detection? In International Conference on Machine\n",
      "Learning (ICML), pages 9934–9944, 2021. 3\n",
      "[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\n",
      "Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\n",
      "Vanhoucke, and Andrew Rabinovich. Going deeper with\n",
      "convolutions. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), pages\n",
      "1–9, 2015. 1, 2, 3\n",
      "[55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\n",
      "Shlens, and Zbigniew Wojna. Rethinking the inception archi-\n",
      "tecture for computer vision. InProceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 2818–2826, 2016. 1\n",
      "[56] Zineng Tang, Jaemin Cho, Jie Lei, and Mohit Bansal.\n",
      "Perceiver-VL: Efficient vision-and-language modeling with\n",
      "iterative latent attention. In Proceedings of the IEEE/CVF\n",
      "Winter Conference on Applications of Computer Vision\n",
      "(WACV), pages 4410–4420, 2023. 1\n",
      "[57] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\n",
      "Fully convolutional one-stage object detection. In Proceed-\n",
      "ings of the IEEE/CVF International Conference on Com-\n",
      "puter Vision (ICCV), pages 9627–9636, 2019. 3\n",
      "[58] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\n",
      "A simple and strong anchor-free object detector. IEEE\n",
      "Transactions on Pattern Analysis and Machine Intelligence\n",
      "(TPAMI), 44(4):1922–1933, 2022. 3\n",
      "[59] Naftali Tishby and Noga Zaslavsky. Deep learning and the\n",
      "information bottleneck principle. In IEEE Information The-\n",
      "ory Workshop (ITW), pages 1–5, 2015. 2, 4\n",
      "[60] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\n",
      "Peyman Milanfar, Alan Bovik, and Yinxiao Li. MaxVIT:\n",
      "Multi-axis vision transformer. In Proceedings of the Euro-\n",
      "pean Conference on Computer Vision (ECCV) , pages 459–\n",
      "479, 2022. 1\n",
      "[61] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo,\n",
      "Chuanjian Liu, Kai Han, and Yunhe Wang. Gold-YOLO:\n",
      "Efficient object detector via gather-and-distribute mecha-\n",
      "nism. Advances in Neural Information Processing Systems\n",
      "(NeurIPS), 2023. 3, 7, 2, 4\n",
      "[62] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\n",
      "Yuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\n",
      "partial network. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 13029–13038, 2021. 3\n",
      "[63] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\n",
      "Yuan Mark Liao. YOLOv7: Trainable bag-of-freebies\n",
      "sets new state-of-the-art for real-time object detectors. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR) , pages 7464–7475,\n",
      "2023. 3, 6, 7, 9, 1\n",
      "[64] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\n",
      "Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A\n",
      "new backbone that can enhance learning capability of CNN.\n",
      "In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition Workshops (CVPRW), pages\n",
      "390–391, 2020. 3, 6, 8\n",
      "[65] Chien-Yao Wang, Hong-Yuan Mark Liao, and I-Hau Yeh.\n",
      "Designing network design strategies through gradient path\n",
      "analysis. Journal of Information Science and Engineering\n",
      "(JISE), 39(4):975–995, 2023. 2, 3, 6\n",
      "[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\n",
      "You only learn one representation: Unified network for mul-\n",
      "tiple tasks. Journal of Information Science & Engineering\n",
      "(JISE), 39(3):691–709, 2023. 2, 3, 4\n",
      "[67] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\n",
      "Sun, and Nanning Zheng. End-to-end object detection\n",
      "with fully convolutional network. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 15849–15858, 2021. 1, 3\n",
      "[68] Liwei Wang, Chen-Yu Lee, Zhuowen Tu, and Svetlana\n",
      "Lazebnik. Training deeper convolutional networks with deep\n",
      "supervision. arXiv preprint arXiv:1505.02496, 2015. 1, 2, 3\n",
      "[69] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\n",
      "Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\n",
      "Pyramid vision transformer: A versatile backbone for dense\n",
      "prediction without convolutions. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision\n",
      "(ICCV), pages 568–578, 2021. 1\n",
      "[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\n",
      "Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. PVT\n",
      "v2: Improved baselines with pyramid vision transformer.\n",
      "Computational Visual Media, 8(3):415–424, 2022. 1\n",
      "[71] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei\n",
      "Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-\n",
      "vNeXt v2: Co-designing and scaling convnets with masked\n",
      "autoencoders. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), pages\n",
      "16133–16142, 2023. 1, 2\n",
      "[72] Saining Xie, Ross Girshick, Piotr Doll ´ar, Zhuowen Tu, and\n",
      "Kaiming He. Aggregated residual transformations for deep\n",
      "neural networks. In Proceedings of the IEEE/CVF Confer-\n",
      "ence on Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 1492–1500, 2017. 1\n",
      "[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\n",
      "Bao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\n",
      "framework for masked image modeling. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 9653–9663, 2022. 2\n",
      "[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\n",
      "Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\n",
      "Dang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\n",
      "evolved version of YOLO.arXiv preprint arXiv:2203.16250,\n",
      "2022. 3, 8, 2, 4\n",
      "[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\n",
      "Yuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\n",
      "port on real-time object detection design. arXiv preprint\n",
      "arXiv:2211.15444, 2022. 3, 7, 2, 4\n",
      "[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\n",
      "Qiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-\n",
      "guided transformer for monocular 3D object detection. In\n",
      "13\n",
      "Proceedings of the IEEE/CVF International Conference on\n",
      "Computer Vision (ICCV), pages 9155–9166, 2023. 1, 3\n",
      "[77] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\n",
      "Ye, and Dongwei Ren. Distance-IoU loss: Faster and bet-\n",
      "ter learning for bounding box regression. In Proceedings of\n",
      "the AAAI Conference on Artificial Intelligence (AAAI) , vol-\n",
      "ume 34, pages 12993–13000, 2020. 1\n",
      "[78] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\n",
      "Yin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\n",
      "object detection. In International Conference on 3D Vision\n",
      "(3DV), pages 85–94, 2019. 1\n",
      "[79] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\n",
      "Songtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\n",
      "entiable label assignment for dense object detection. arXiv\n",
      "preprint arXiv:2007.03496, 2020. 1\n",
      "[80] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang,\n",
      "Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient\n",
      "visual representation learning with bidirectional state space\n",
      "model. arXiv preprint arXiv:2401.09417, 2024. 1\n",
      "[81] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng\n",
      "Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-\n",
      "training unified architecture for generic perception for zero-\n",
      "shot and few-shot tasks. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 16804–16815, 2022. 1\n",
      "[82] Zhuofan Zong, Guanglu Song, and Yu Liu. DETRs with\n",
      "collaborative hybrid assignments training. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR), pages 6748–6758, 2023. 3\n",
      "14\n",
      "Appendix\n",
      "A. Implementation Details\n",
      "Table 1. Hyper parameter settings of YOLOv9.\n",
      "hyper parameter value\n",
      "epochs 500\n",
      "optimizer SGD\n",
      "initial learning rate 0.01\n",
      "finish learning rate 0.0001\n",
      "learning rate decay linear\n",
      "momentum 0.937\n",
      "weight decay 0.0005\n",
      "warm-up epochs 3\n",
      "warm-up momentum 0.8\n",
      "warm-up bias learning rate 0.1\n",
      "box loss gain 7.5\n",
      "class loss gain 0.5\n",
      "DFL loss gain 1.5\n",
      "HSV saturation augmentation 0.7\n",
      "HSV value augmentation 0.4\n",
      "translation augmentation 0.1\n",
      "scale augmentation 0.9\n",
      "mosaic augmentation 1.0\n",
      "MixUp augmentation 0.15\n",
      "copy & paste augmentation 0.3\n",
      "close mosaic epochs 15\n",
      "The training parameters of YOLOv9 are shown in Ta-\n",
      "ble 1. We fully follow the settings of YOLOv7 AF [63],\n",
      "which is to use SGD optimizer to train 500 epochs. We first\n",
      "warm-up for 3 epochs and only update the bias during the\n",
      "warm-up stage. Next we step down from the initial learning\n",
      "rate 0.01 to 0.0001 in linear decay manner, and the data aug-\n",
      "mentation settings are listed in the bottom part of Table 1.\n",
      "We shut down mosaic data augmentation operations on the\n",
      "last 15 epochs.\n",
      "Table 2. Network configurations of YOLOv9.\n",
      "Index Module Route Filters Depth Size Stride\n",
      "0 Conv – 64 – 3 2\n",
      "1 Conv 0 128 – 3 2\n",
      "2 CSP-ELAN 1 256, 128, 64 2, 1 – 1\n",
      "3 DOWN 2 256 – 3 2\n",
      "4 CSP-ELAN 3 512, 256, 128 2, 1 – 1\n",
      "5 DOWN 4 512 – 3 2\n",
      "6 CSP-ELAN 5 512, 512, 256 2, 1 – 1\n",
      "7 DOWN 6 512 – 3 2\n",
      "8 CSP-ELAN 7 512, 512, 256 2, 1 – 1\n",
      "9 SPP-ELAN 8 512, 256, 256 3, 1 – 1\n",
      "10 Up 9 512 – – 2\n",
      "11 Concat 10, 6 1024 – – 1\n",
      "12 CSP-ELAN 11 512, 512, 256 2, 1 – 1\n",
      "13 Up 12 512 – – 2\n",
      "14 Concat 13, 4 1024 – – 1\n",
      "15 CSP-ELAN 14 256, 256, 128 2, 1 – 1\n",
      "16 DOWN 15 256 – 3 2\n",
      "17 Concat 16, 12 768 – – 1\n",
      "18 CSP-ELAN 17 512, 512, 256 2, 1 – 1\n",
      "19 DOWN 18 512 – 3 2\n",
      "20 Concat 19, 9 1024 – – 1\n",
      "21 CSP-ELAN 20 512, 512, 256 2, 1 – 1\n",
      "22 Predict 15, 18, 21 – – – –\n",
      "The network topology of YOLOv9 completely follows\n",
      "YOLOv7 AF [63], that is, we replace ELAN with the pro-\n",
      "posed CSP-ELAN block. As listed in Table 2, the depth\n",
      "parameters of CSP-ELAN are represented as ELAN depth\n",
      "and CSP depth, respectively. As for the parameters of CSP-\n",
      "ELAN filters, they are represented as ELAN output fil-\n",
      "ter, CSP output filter, and CSP inside filter. In the down-\n",
      "sampling module part, we simplify CSP-DOWN module to\n",
      "DOWN module. DOWN module is composed of a pooling\n",
      "layer with size 2 and stride 1, and a Conv layer with size 3\n",
      "and stride 2. Finally, we optimized the prediction layer and\n",
      "replaced top, left, bottom, and right in the regression branch\n",
      "with decoupled branch.\n",
      "1\n",
      "Table 3. Comparison of state-of-the-art object detectors with different training settings.\n",
      "Model #Param. (M) FLOPs (G) AP 50:95 (%) AP 50 (%) AP 75 (%) AP S (%) AP M (%) AP L (%)\n",
      "Train-from-scratch\n",
      "Dy-YOLOv7 [36] – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\n",
      "Dy-YOLOv7-X [36] – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\n",
      "YOLOv9-S (Ours) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\n",
      "YOLOv9-M (Ours) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\n",
      "YOLOv9-C (Ours) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\n",
      "YOLOv9-E (Ours) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\n",
      "YOLOv9-E (Ours) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\n",
      "YOLOv9-E (Ours) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\n",
      "ImageNet Pretrained\n",
      "RTMDet-T [44] 4.8 12.6 41.1 57.9 – – – –\n",
      "RTMDet-S [44] 9.0 25.6 44.6 61.9 – – – –\n",
      "RTMDet-M [44] 24.7 78.6 49.4 66.8 – – – –\n",
      "RTMDet-L [44] 52.3 160.4 51.5 68.8 – – – –\n",
      "RTMDet-X [44] 94.9 283.4 52.8 70.4 – – – –\n",
      "PPYOLOE-S [74] 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\n",
      "PPYOLOE-M [74] 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\n",
      "PPYOLOE-L [74] 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\n",
      "PPYOLOE-X [74] 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\n",
      "RT DETR-L [43] 32 110 53.0 71.6 57.3 34.6 57.3 71.2\n",
      "RT DETR-X [43] 67 234 54.8 73.1 59.4 35.7 59.6 72.9\n",
      "RT DETR-R18 [43] 20 60 46.5 63.8 – – – –\n",
      "RT DETR-R34 [43] 31 92 48.9 66.8 – – – –\n",
      "RT DETR-R50M [43] 36 100 51.3 69.6 – – – –\n",
      "RT DETR-R50 [43] 42 136 53.1 71.3 57.7 34.8 58.0 70.0\n",
      "RT DETR-R101 [43] 76 259 54.3 72.7 58.6 36.0 58.8 72.1\n",
      "Gold YOLO-S [61] 21.5 46.0 45.5 62.2 – – – –\n",
      "Gold YOLO-M [61] 41.3 57.5 50.2 67.5 – – – –\n",
      "Gold YOLO-L [61] 75.1 151.7 52.3 69.6 – – – –\n",
      "Knowledge Distillation\n",
      "YOLOv6-N v3.0 [30] 4.7 11.4 37.5 53.1 – – – –\n",
      "YOLOv6-S v3.0 [30] 18.5 45.3 45.0 61.8 – – – –\n",
      "YOLOv6-M v3.0 [30] 34.9 85.8 50.0 66.9 – – – –\n",
      "YOLOv6-L v3.0 [30] 59.6 150.7 52.8 70.3 – – – –\n",
      "DAMO YOLO-T [75] 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\n",
      "DAMO YOLO-S [75] 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\n",
      "DAMO YOLO-M [75] 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\n",
      "DAMO YOLO-L [75] 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\n",
      "Gold YOLO-N [61] 5.6 12.1 39.9 55.9 – – – –\n",
      "Gold YOLO-S [61] 21.5 46.0 46.1 63.3 – – – –\n",
      "Gold YOLO-M [61] 41.3 57.5 50.9 68.2 – – – –\n",
      "Gold YOLO-L [61] 75.1 151.7 53.2 70.5 – – – –\n",
      "Complex Setting\n",
      "Gold YOLO-S [61] 21.5 46.0 46.4 63.4 – – – –\n",
      "Gold YOLO-M [61] 41.3 57.5 51.1 68.5 – – – –\n",
      "Gold YOLO-L [61] 75.1 151.7 53.3 70.9 – – – –\n",
      "YOLOR-CSP [66] 52.9 120.4 52.8 71.2 57.6 – – –\n",
      "YOLOR-CSP-X [66] 96.9 226.8 54.8 73.1 59.7 – – –\n",
      "PPYOLOE+-S [74] 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\n",
      "PPYOLOE+-M [74] 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\n",
      "PPYOLOE+-L [74] 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\n",
      "PPYOLOE+-X [74] 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\n",
      "B. More Comparison\n",
      "We compare YOLOv9 to state-of-the-art real-time object\n",
      "detectors trained with different methods. It mainly includes\n",
      "four different training methods: (1) train-from-scratch: we\n",
      "have completed most of the comparisons in the text. Here\n",
      "are only list of additional data of DynamicDet [36] for com-\n",
      "parisons; (2) Pretrained by ImageNet: this includes two\n",
      "methods of using ImageNet for supervised pretrain and self-\n",
      "supervised pretrain; (3) knowledge distillation: a method\n",
      "to perform additional self-distillation after training is com-\n",
      "pleted; and (4) a more complex training process: a combi-\n",
      "nation of steps including pretrained by ImageNet, knowl-\n",
      "edge distillation, DAMO-YOLO and even additional pre-\n",
      "trained large object detection dataset. We show the results\n",
      "in Table 3. From this table, we can see that our proposed\n",
      "YOLOv9 performed better than all other methods. Com-\n",
      "pared with PPYOLOE+-X trained using ImageNet and Ob-\n",
      "jects365, our method still reduces the number of parame-\n",
      "ters by 55% and the amount of computation by 11%, and\n",
      "improving 0.4% AP.\n",
      "2\n",
      "Table 4. Comparison of state-of-the-art object detectors with different training settings (sorted by number of parameters).\n",
      "Model #Param. (M) FLOPs (G) AP val\n",
      "50:95 (%) AP val\n",
      "50 (%) AP val\n",
      "75 (%) AP val\n",
      "S (%) AP val\n",
      "M (%) AP val\n",
      "L (%)\n",
      "YOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\n",
      "RTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\n",
      "Gold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\n",
      "YOLOv9-S (S) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\n",
      "PPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\n",
      "PPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\n",
      "DAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\n",
      "RTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\n",
      "DAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\n",
      "YOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\n",
      "RT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\n",
      "YOLOv9-M (S) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\n",
      "Gold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\n",
      "Gold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\n",
      "Gold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\n",
      "PPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\n",
      "PPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\n",
      "RTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\n",
      "YOLOv9-C (S) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\n",
      "DAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\n",
      "RT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\n",
      "RT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\n",
      "YOLOv9-E (S) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\n",
      "YOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\n",
      "RT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\n",
      "Gold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\n",
      "Gold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\n",
      "Gold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\n",
      "RT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\n",
      "DAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\n",
      "YOLOv9-E (S) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\n",
      "PPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\n",
      "PPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\n",
      "RTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\n",
      "YOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\n",
      "YOLOv9-E (S) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\n",
      "YOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\n",
      "RT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\n",
      "Gold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\n",
      "Gold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\n",
      "Gold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\n",
      "RT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\n",
      "RTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\n",
      "YOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\n",
      "PPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\n",
      "PPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\n",
      "1 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\n",
      "Table 4 shows the performance of all models sorted by\n",
      "parameter size. Our proposed YOLOv9 is Pareto optimal\n",
      "in all models of different sizes. Among them, we found no\n",
      "other method for Pareto optimal in models with more than\n",
      "20M parameters. The above experimental data shows that\n",
      "our YOLOv9 has excellent parameter usage efficiency.\n",
      "Shown in Table 5 is the performance of all participat-\n",
      "ing models sorted by the amount of computation. Our pro-\n",
      "posed YOLOv9 is Pareto optimal in all models with differ-\n",
      "ent scales. Among models with more than 60 GFLOPs, only\n",
      "ELAN-based DAMO-YOLO and DETR-based RT DETR\n",
      "can rival the proposed YOLOv9. The above comparison\n",
      "results show that YOLOv9 has the most outstanding per-\n",
      "formance in the trade-off between computation complexity\n",
      "and accuracy.\n",
      "3\n",
      "Table 5. Comparison of state-of-the-art object detectors with different training settings (sorted by amount of computation).\n",
      "Model #Param. (M) FLOPs (G) AP val\n",
      "50:95 (%) AP val\n",
      "50 (%) AP val\n",
      "75 (%) AP val\n",
      "S (%) AP val\n",
      "M (%) AP val\n",
      "L (%)\n",
      "YOLOv6-N v3.0 [30] (D) 4.7 11.4 37.5 53.1 – – – –\n",
      "Gold YOLO-N [61] (D) 5.6 12.1 39.9 55.9 – – – –\n",
      "RTMDet-T [44] (I) 4.8 12.6 41.1 57.9 – – – –\n",
      "PPYOLOE+-S [74] (C) 7.9 14.4 43.7 60.6 47.9 23.2 46.4 56.9\n",
      "PPYOLOE-S [74] (I) 7.9 14.4 43.0 60.5 46.6 23.2 46.4 56.9\n",
      "DAMO YOLO-T [75] (D) 8.5 18.1 43.6 59.4 46.6 23.3 47.4 61.0\n",
      "RTMDet-S [44] (I) 9.0 25.6 44.6 61.9 – – – –\n",
      "YOLOv9-S (S) 7.1 26.4 46.8 63.4 50.7 26.6 56.0 64.5\n",
      "DAMO YOLO-S [75] (D) 16.3 37.8 47.7 63.5 51.1 26.9 51.7 64.9\n",
      "YOLOv6-S v3.0 [30] (D) 18.5 45.3 45.0 61.8 – – – –\n",
      "Gold YOLO-S [61] (C) 21.5 46.0 46.4 63.4 – – – –\n",
      "Gold YOLO-S [61] (D) 21.5 46.0 46.1 63.3 – – – –\n",
      "Gold YOLO-S [61] (I) 21.5 46.0 45.5 62.2 – – – –\n",
      "PPYOLOE+-M [74] (C) 23.4 49.9 49.8 67.1 54.5 31.8 53.9 66.2\n",
      "PPYOLOE-M [74] (I) 23.4 49.9 49.0 66.5 53.0 28.6 52.9 63.8\n",
      "Gold YOLO-M [61] (C) 41.3 57.5 51.1 68.5 – – – –\n",
      "Gold YOLO-M [61] (D) 41.3 57.5 50.9 68.2 – – – –\n",
      "Gold YOLO-M [61] (I) 41.3 57.5 50.2 67.5 – – – –\n",
      "RT DETR-R18 [43] (I) 20 60 46.5 63.8 – – – –\n",
      "DAMO YOLO-M [75] (D) 28.2 61.8 50.4 67.2 55.1 31.6 55.3 67.1\n",
      "YOLOv9-M (S) 20.0 76.3 51.4 68.1 56.1 33.6 57.0 68.0\n",
      "RTMDet-M [44] (I) 24.7 78.6 49.4 66.8 – – – –\n",
      "YOLOv6-M v3.0 [30] (D) 34.9 85.8 50.0 66.9 – – – –\n",
      "RT DETR-R34 [43] (I) 31 92 48.9 66.8 – – – –\n",
      "DAMO YOLO-L [75] (D) 42.1 97.3 51.9 68.5 56.7 33.3 57.0 67.6\n",
      "RT DETR-R50M [43] (I) 36 100 51.3 69.6 – – – –\n",
      "YOLOv9-C (S) 25.3 102.1 53.0 70.2 57.8 36.2 58.5 69.3\n",
      "RT DETR-L [43] (I) 32 110 53.0 71.6 57.3 34.6 57.3 71.2\n",
      "PPYOLOE+-L [74] (C) 52.2 110.1 52.9 70.1 57.9 35.2 57.5 69.1\n",
      "PPYOLOE-L [74] (I) 52.2 110.1 51.4 68.9 55.6 31.4 55.3 66.1\n",
      "YOLOR-CSP [66] (C) 52.9 120.4 52.8 71.2 57.6 – – –\n",
      "RT DETR-R50 [43] (I) 42 136 53.1 71.3 57.7 34.8 58.0 70.0\n",
      "YOLOv9-E (S) 34.7 147.1 54.5 71.7 59.2 38.1 59.9 70.3\n",
      "YOLOv6-L v3.0 [30] (D) 59.6 150.7 52.8 70.3 – – – –\n",
      "Gold YOLO-L [61] (C) 75.1 151.7 53.3 70.9 – – – –\n",
      "Gold YOLO-L [61] (D) 75.1 151.7 53.2 70.5 – – – –\n",
      "Gold YOLO-L [61] (I) 75.1 151.7 52.3 69.6 – – – –\n",
      "RTMDet-L [44] (I) 52.3 160.4 51.5 68.8 – – – –\n",
      "Dy-YOLOv7 [36] (S) – 181.7 53.9 72.2 58.7 35.3 57.6 66.4\n",
      "YOLOv9-E (S) 44.0 183.9 55.1 72.3 60.7 38.7 60.6 71.4\n",
      "YOLOv9-E (S) 57.3 189.0 55.6 72.8 60.6 40.2 61.0 71.4\n",
      "PPYOLOE+-X [74] (C) 98.4 206.6 54.7 72.0 59.9 37.9 59.3 70.4\n",
      "PPYOLOE-X [74] (I) 98.4 206.6 52.3 69.5 56.8 35.1 57.0 68.6\n",
      "YOLOR-CSP-X [66] (C) 96.9 226.8 54.8 73.1 59.7 – – –\n",
      "RT DETR-X [43] (I) 67 234 54.8 73.1 59.4 35.7 59.6 72.9\n",
      "RT DETR-R101 [43] (I) 76 259 54.3 72.7 58.6 36.0 58.8 72.1\n",
      "RTMDet-X [44] (I) 94.9 283.4 52.8 70.4 – – – –\n",
      "Dy-YOLOv7-X [36] (S) – 307.9 55.0 73.2 60.0 36.6 58.7 68.5\n",
      "1 (S), (I), (D), (C) indicate train-from-scratch, ImageNet pretrained, knowledge distillation, and complex setting, respectively.\n",
      "4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lấy nội dung toàn bộ các trang\n",
    "pages_txt = \"\"\n",
    "for i in range(num_pages):\n",
    "    page = reader.pages[i]\n",
    "    pages_txt += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(pages_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im1.png\n",
      "Im2.png\n",
      "Im3.png\n",
      "Im4.png\n",
      "Im5.png\n",
      "Im6.png\n",
      "Im7.png\n"
     ]
    }
   ],
   "source": [
    "# Extract images\n",
    "count = 0\n",
    "for page in reader.pages:\n",
    "    for image_file_object in page.images:\n",
    "        with open(str(count) + image_file_object.name, \"wb\") as fp:\n",
    "            fp.write(image_file_object.data)\n",
    "            count += 1\n",
    "        print(image_file_object.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63fd5ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"17.0.6\" 2023-01-17 LTS\n",
      "Java(TM) SE Runtime Environment (build 17.0.6+9-LTS-190)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 17.0.6+9-LTS-190, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acaed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tên</th>\n",
       "      <th>Lớp</th>\n",
       "      <th>Điểm Trung Bình</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anh</td>\n",
       "      <td>10A</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bình</td>\n",
       "      <td>10B</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Châu</td>\n",
       "      <td>10C</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tên  Lớp  Điểm Trung Bình\n",
       "0   Anh  10A              8.5\n",
       "1  Bình  10B              7.2\n",
       "2  Châu  10C              9.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./data/tabula-py1.pdf\"\n",
    "dfs = tabula.read_pdf(pdf_path, pages=\"1\", lattice=True)\n",
    "print(len(dfs))\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebfb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
